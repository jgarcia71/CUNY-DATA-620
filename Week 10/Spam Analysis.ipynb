{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10/11 Assignment: Document Classification\n",
    "\n",
    "## Assignment Description\n",
    "\n",
    "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  Here is one example of such data:  [http://archive.ics.uci.edu/ml/datasets/Spambase](http://archive.ics.uci.edu/ml/datasets/Spambase)\n",
    "\n",
    "For this project, you can either use the above dataset to predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).\n",
    "\n",
    "For more adventurous students, you are welcome (encouraged!) to come up a different set of documents (including scraped web pages!?) that have already been classified (e.g. tagged), then analyze these documents to predict how new documents should be classified.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "We can get information about the dataset from [http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names](http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names).  I've copied the most pertinent parts of that file here:\n",
    "\n",
    "48 continuous real [0,100] attributes of type word_freq_WORD \n",
    "= percentage of words in the e-mail that match WORD,\n",
    "i.e. 100 * (number of times the WORD appears in the e-mail) / total number of words in e-mail.  A \"word\" in this case is any string of alphanumeric characters bounded by non-alphanumeric characters or end-of-string.\n",
    "\n",
    "6 continuous real [0,100] attributes of type char_freq_CHAR\n",
    "= percentage of characters in the e-mail that match CHAR,\n",
    "i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
    "\n",
    "1 continuous real [1,...] attribute of type capital_run_length_average\n",
    "= average length of uninterrupted sequences of capital letters\n",
    "\n",
    "1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
    "= length of longest uninterrupted sequence of capital letters\n",
    "\n",
    "1 continuous integer [1,...] attribute of type capital_run_length_total\n",
    "= sum of length of uninterrupted sequences of capital letters\n",
    "= total number of capital letters in the e-mail\n",
    "\n",
    "1 nominal {0,1} class attribute of type spam\n",
    "= denotes whether the e-mail was considered spam (1) or not (0), \n",
    "i.e. unsolicited commercial e-mail.  \n",
    "\n",
    "## Get Data\n",
    "\n",
    "Data is found, without headers, in [http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data](http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data). \n",
    "\n",
    "Header names can be found in [http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names](http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64             0   \n",
       "1            0.21               0.28           0.50             0   \n",
       "2            0.06               0.00           0.71             0   \n",
       "3            0.00               0.00           0.00             0   \n",
       "4            0.00               0.00           0.00             0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...   char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...          0.00        0.000   \n",
       "1             0.00            0.94  ...          0.00        0.132   \n",
       "2             0.64            0.25  ...          0.01        0.143   \n",
       "3             0.31            0.63  ...          0.00        0.137   \n",
       "4             0.31            0.63  ...          0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0            0        0.778        0.000        0.000   \n",
       "1            0        0.372        0.180        0.048   \n",
       "2            0        0.276        0.184        0.010   \n",
       "3            0        0.137        0.000        0.000   \n",
       "4            0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                       278     1  \n",
       "1                      1028     1  \n",
       "2                      2259     1  \n",
       "3                       191     1  \n",
       "4                       191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\",\\\n",
    "         \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\",\\\n",
    "         \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\",\\\n",
    "         \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \\\n",
    "         \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \\\n",
    "         \"word_freq_credit\", \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \\\n",
    "         \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\",\\\n",
    "         \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\", \\\n",
    "         \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \\\n",
    "         \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\\\n",
    "         \"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\",\\\n",
    "         \"word_freq_project\", \"word_freq_re\", \"word_freq_edu\", \"word_freq_table\",\\\n",
    "         \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \\\n",
    "         \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \"capital_run_length_average\", \\\n",
    "         \"capital_run_length_longest\", \"capital_run_length_total\", \"spam\"]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "email_data = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\", \\\n",
    "                        names = names)\n",
    "\n",
    "email_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Analysis\n",
    "\n",
    "In the case of this data, it comes from a specific time frame that might not be reflective of spam habits today, so let's create and test a model on data taken entirely from this data.  We'll want both a training and a testing set in order to do this properly.\n",
    "\n",
    "First off, how many rows of data do we have overall?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4601"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(email_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With over 4600 cases, we should be able to get a great result and we can even set aside some for a \"preliminary test set\".  Let's set aside about 10% each for testing and prelim testing, and the rest can be training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train', 'train', 'test', 'train', 'train', 'train', 'train',\n",
       "       'train', 'prelim_test', 'train', 'train', 'prelim_test', 'train',\n",
       "       'train', 'train', 'test', 'train', 'train', 'train', 'train'], \n",
       "      dtype='|S11')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "assignments = np.random.choice((\"test\", \"prelim_test\", \"train\"), p=[0.1, 0.1, 0.8], size=len(email_data))\n",
    "assignments[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set, test_set, prelim_test_set =  email_data[assignments == \"train\"], \\\n",
    "                                        email_data[assignments == \"test\"],\\\n",
    "                                        email_data[assignments == \"prelim_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a Bit of Visualization\n",
    "\n",
    "Let's take a peek at our training set to get an idea for some trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11963e590>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAERCAYAAACO6FuTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119622d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "sns.boxplot(y='capital_run_length_total', x='spam', data=email_data, showfliers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, for example, we can see that spam tends to have a higher total capital letter run length than non-spam.  That makes sense when you consider the tone of spam which often has URGENT, EXCITING information to INCITE you to BUY NOW!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for ML\n",
    "\n",
    "In order to prepare for the use of machine learning, it makes sense to consider what we want for our optimal outcome.\n",
    "\n",
    "For example, it isn't a huge deal if a few spam messages make it through to someone's inbox, but it could be bad for business if a legit email made it into the the spam folder and didn't get answered.  So let's say that accuracy on non-spam is twice as important as accuracy on spam!  We can use class weights (where available) to make sure that the models we generate are biasing the model the way we want -- to be really really good at classifying non-spam, even if that means a few spam emails make it through the filter.\n",
    "\n",
    "Note that we'll want to consider this weighting as well when we calculate our metrics for evaluating a model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit Learn\n",
    "\n",
    "Let's start with a nearest neighbor algorithm and check out its confusion matrix and accuracy.  We'll take the outcome variable (spam), which is the last column in our training set, and call it \"train_truth\", while the rest of the variables in our training set become \"train_predictors\".  Then we'll run a fit where we use k nearest neighbors to create a model, and then use that model to make a prediction, which we can compare to the truth.\n",
    "\n",
    "### k Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "train_truth = train_set.iloc[:,-1:].values.ravel()\n",
    "train_predictors = train_set.iloc[:,:-1]\n",
    "knn.fit(train_predictors, train_truth)\n",
    "knn_prediction = knn.predict(train_predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not terribly fond of the confusion matrix native to sklearn, so I'm using a pandas crosstab instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1994</td>\n",
       "      <td>240</td>\n",
       "      <td>2234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>260</td>\n",
       "      <td>1195</td>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>2254</td>\n",
       "      <td>1435</td>\n",
       "      <td>3689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1   All\n",
       "True                       \n",
       "0          1994   240  2234\n",
       "1           260  1195  1455\n",
       "All        2254  1435  3689"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = pd.crosstab(train_truth, knn_prediction, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want some accuracy metrics.  Since our sample is unbalanced (1353 spam to 2788 non-spam), we'd love to have balanced accuracy, but balanced\\_accuracy\\_score doesn't come out until the next version of sklearn.  Luckily, classification reporting from sklearn is really helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Accuracy\n",
    "\n",
    "What kind of accuracy do we want?  \n",
    "\n",
    "__Precision__, or the number of true positives divided by true and false positives, is the same thing we would call \"Positive Predictive Value\".  How likely are we to be right when we rule \"that's spam\" or \"that's not spam\"?  It doesn't penalize missing some cases.  For example, if I only said \"that's spam\" for the most egregious cases, I could be sure that my precision for spam was 100% -- every time I say it, it turns out I'm right.  Of course once I move to the score for \"that's not spam\" the problem with this approach becomes clear, as my metrics for the other class are really low.\n",
    "\n",
    "__Recall__, or the number of true positives divided by true positives and false negatives, is also known as sensitivity.  It measures how much of a given class is correctly picked up and doesn't penalize over-enthusiasm (e.g. if I said \"it's all spam!\" then my recall for the spam class would be 100%).  Again, a sloppy measure like that would be penalized once I looked at the recall for non-spam.\n",
    "\n",
    "__F1 Score__, or  2\\*((precision\\*recall)/(precision+recall)), balances precision and recall.\n",
    "\n",
    "Overall \"__accuracy__\" is the number of true positives and true negatives (accurate classifications) divided by all classifications.\n",
    "\n",
    "__Balanced accuracy__ takes accuracy for each class and averages them.\n",
    "\n",
    "We may also want to come up with weighted metrics or our own methods for coming up with an accuracy metric.  Let's see what comes out of the box of scikit-learn first, though!  Note here that `classification_report` is just a pretty-print version of `precision_recall_fscore_support`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prettified Classification Report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.89      0.89      2234\n",
      "          1       0.83      0.82      0.83      1455\n",
      "\n",
      "avg / total       0.86      0.86      0.86      3689\n",
      "\n",
      "\n",
      "Ugly Report:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.88464951,  0.83275261]),\n",
       " array([ 0.89256938,  0.82130584]),\n",
       " array([ 0.8885918 ,  0.82698962]),\n",
       " array([2234, 1455]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print \"\\nPrettified Classification Report: \\n\\n\" + str(classification_report(train_truth, knn_prediction)) \n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "print \"\\nUgly Report:\" \n",
    "precision_recall_fscore_support(train_truth, knn_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Accuracy Metric\n",
    "\n",
    "Now that we have some idea of the accuracy metrics available to us, let's create a custom accuracy metric that weights non-spam higher than spam (because we care more about getting non-spam right).  We'll use positive predictive value, or the precision score, taking two of the f1-score for non-spam (classification = 0) and one of the f1 score for spam (classification = 1) and dividing by three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def custom_accuracy (report):\n",
    "    ppv_non_spam = report[0][0] # precision = first row -- so [0], non-spam = first col, so [0]\n",
    "    ppv_spam = report[0][1]\n",
    "    return ((2*ppv_non_spam) + ppv_spam)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try our custom accuracy on our knn model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86735054573260906"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_accuracy = custom_accuracy(precision_recall_fscore_support(train_truth, knn_prediction))\n",
    "knn_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby, but we remember that we're certainly overfitting a bit, so this accuracy will go down in test.  What happens in prelim_test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80688552004944203"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prelim_truth = prelim_test_set.iloc[:,-1:].values.ravel()\n",
    "prelim_predictors = prelim_test_set.iloc[:,:-1]\n",
    "prelim_knn_prediction = knn.predict(prelim_predictors)\n",
    "custom_accuracy(precision_recall_fscore_support(prelim_truth, prelim_knn_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ugh, that's a big drop... Let's try a few more algorithms to see if we get better performance.\n",
    "\n",
    "What about a logistic regression?\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "One cool thing about logistic regression is that we can add class weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression(class_weight = {0:2,1:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight={0: 2, 1: 1}, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.fit(train_predictors, train_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_prediction = logistic.predict(train_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2168</td>\n",
       "      <td>66</td>\n",
       "      <td>2234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>268</td>\n",
       "      <td>1187</td>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>2436</td>\n",
       "      <td>1253</td>\n",
       "      <td>3689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1   All\n",
       "True                       \n",
       "0          2168    66  2234\n",
       "1           268  1187  1455\n",
       "All        2436  1253  3689"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(train_truth, logistic_prediction, rownames=['True'], colnames=['Predicted'], margins=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90909785862588788"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_accuracy = custom_accuracy(precision_recall_fscore_support(train_truth, logistic_prediction))\n",
    "logistic_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much higher accuracy!  Again, let's do some prelim testing to see if this improved accuracy holds up in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90858465822122747"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prelim_logistic_prediction = logistic.predict(prelim_predictors)\n",
    "custom_accuracy(precision_recall_fscore_support(prelim_truth, prelim_logistic_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a much more robust algorithm than knn -- a smaller change, which indicates less overfitting. \n",
    "\n",
    "But let's keep going with some additional algorithms.  Maybe a cool fancy neural network?\n",
    "\n",
    "### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(train_predictors, train_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2164</td>\n",
       "      <td>70</td>\n",
       "      <td>2234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166</td>\n",
       "      <td>1289</td>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>2330</td>\n",
       "      <td>1359</td>\n",
       "      <td>3689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1   All\n",
       "True                       \n",
       "0          2164    70  2234\n",
       "1           166  1289  1455\n",
       "All        2330  1359  3689"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_prediction = mlp.predict(train_predictors)\n",
    "pd.crosstab(train_truth, mlp_prediction, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93533408916974847"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_accuracy = custom_accuracy(precision_recall_fscore_support(train_truth, mlp_prediction))\n",
    "mlp_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one's pretty good.  Does it hold up to preliminary testing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94199134199134205"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prelim_mlp_prediction = mlp.predict(prelim_predictors)\n",
    "custom_accuracy(precision_recall_fscore_support(prelim_truth, prelim_mlp_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice small change.  This one's a keeper!\n",
    "\n",
    "But we haven't done a tree or forest algorithm yet.  I'll toss one in for good measure:\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "Decision Trees are another place where we can take advantage of class weights to bias the model to favor accuracy for non-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight={0: 2, 1: 1}, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(class_weight = {0:2,1:1})\n",
    "tree.fit(train_predictors, train_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2234</td>\n",
       "      <td>0</td>\n",
       "      <td>2234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1453</td>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>2236</td>\n",
       "      <td>1453</td>\n",
       "      <td>3689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1   All\n",
       "True                       \n",
       "0          2234     0  2234\n",
       "1             2  1453  1455\n",
       "All        2236  1453  3689"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_prediction = tree.predict(train_predictors)\n",
    "pd.crosstab(train_truth, tree_prediction, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99940369707811572"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_accuracy = custom_accuracy(precision_recall_fscore_support(train_truth, tree_prediction))\n",
    "tree_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy mackerel!  This is very accurate.  Again, we have to be careful about the danger of overfitting, so let's not get too psyched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91971340130311496"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prelim_tree_prediction = tree.predict(prelim_predictors)\n",
    "custom_accuracy(precision_recall_fscore_support(prelim_truth, prelim_tree_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a drop, for sure, but it's still quite accurate.  I've got one fantastic predictor (tree) and two pretty good ones (Logistic and MLP).  Since these three are outperforming the knn, I'll discard it.  Can I ensemble these three to get some sort of super-model that is greater than the sum of its parts?  I can add the predictions from the three good models to my existing predictors and maybe do a random forest on that?\n",
    "\n",
    "### Ensembling Successful Models\n",
    "\n",
    "I'll set up both my training and prelim testing sets with the outputs of the three successful models as columns, and try an algorithm on this combined data -- this is an ensemble method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_predictors['logistic_prediction'] = logistic_prediction\n",
    "train_predictors['mlp_prediction'] = mlp_prediction\n",
    "train_predictors['tree_prediction'] = tree_prediction\n",
    "\n",
    "prelim_predictors['logistic_prediction'] = prelim_logistic_prediction\n",
    "prelim_predictors['mlp_prediction'] = prelim_mlp_prediction\n",
    "prelim_predictors['tree_prediction'] = prelim_tree_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2233</td>\n",
       "      <td>1</td>\n",
       "      <td>2234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1454</td>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>2234</td>\n",
       "      <td>1455</td>\n",
       "      <td>3689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1   All\n",
       "True                       \n",
       "0          2233     1  2234\n",
       "1             1  1454  1455\n",
       "All        2234  1455  3689"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(class_weight = {0:2,1:1})\n",
    "rf.fit(train_predictors, train_truth)\n",
    "rf_prediction = rf.predict(train_predictors)\n",
    "pd.crosstab(train_truth, rf_prediction, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out accuracy -- first on the training, then on the prelim test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99947248654297172"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_accuracy(precision_recall_fscore_support(train_truth, rf_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92241428983002027"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prelim_rf_prediction = rf.predict(prelim_predictors)\n",
    "custom_accuracy(precision_recall_fscore_support(prelim_truth, prelim_rf_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we managed to squeeze out a bit more performance using the ensemble method.  Let's call this our classifier!\n",
    "\n",
    "To recap the method:\n",
    "\n",
    "- use logistic, mlp, and tree models to come up with predictions.\n",
    "- add those predictions as additional predictors \n",
    "- run a final random forest algorithm on this final set of predictors.\n",
    "\n",
    "## Test The Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.920479302832244"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_truth = test_set.iloc[:,-1:].values.ravel()\n",
    "test_predictors = test_set.iloc[:,:-1]\n",
    "\n",
    "test_logistic_prediction = logistic.predict(test_predictors)\n",
    "test_mlp_prediction = mlp.predict(test_predictors)\n",
    "test_tree_prediction = tree.predict(test_predictors)\n",
    "\n",
    "test_predictors['logistic_prediction'] = test_logistic_prediction\n",
    "test_predictors['mlp_prediction'] = test_mlp_prediction\n",
    "test_predictors['tree_prediction'] = test_tree_prediction\n",
    "\n",
    "test_rf_prediction = rf.predict(test_predictors)\n",
    "custom_accuracy(precision_recall_fscore_support(test_truth, test_rf_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, we feel pretty happy with a 92% accuracy rate, as defined by our custom accuracy metric!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
